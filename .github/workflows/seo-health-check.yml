name: SEO Health Check

on:
  schedule:
    - cron: '0 6 * * 1'  # Weekly on Monday at 6 AM UTC
  workflow_dispatch:  # Allow manual trigger
  push:
    branches: [main]
    paths:
      - '_config.yml'
      - '_layouts/**'
      - '_posts/**'
      - 'robots.txt'

permissions:
  contents: read
  actions: write

jobs:
  seo-check:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v5
        
      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.2'
          bundler-cache: true
          
      - name: Install XML tools
        run: sudo apt-get update && sudo apt-get install -y libxml2-utils
          
      - name: Build Jekyll site
        run: bundle exec jekyll build
        
      - name: Serve site for Lighthouse
        run: |
          bundle exec jekyll serve --detach --port 4000
          sleep 5  # Wait for server to start
          
      - name: Run Lighthouse CI
        uses: treosh/lighthouse-ci-action@v12
        with:
          urls: |
            http://localhost:4000
            http://localhost:4000/about/
            http://localhost:4000/archive/
          configPath: './.lighthouserc.json'
          uploadArtifacts: false
          temporaryPublicStorage: true
          
      - name: Upload Lighthouse results
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          echo "LIGHTHOUSE_ARTIFACT_NAME=lighthouse-results-$TIMESTAMP" >> $GITHUB_ENV
        
      - name: Upload Lighthouse artifacts
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.LIGHTHOUSE_ARTIFACT_NAME }}
          path: .lighthouseci/
          retention-days: 90
        
      - name: Check canonical URLs consistency
        run: |
          echo "Checking canonical URLs..."
          INCONSISTENT=$(find _site -name "*.html" -exec grep -l "canonical" {} \; | xargs grep "canonical" | grep -v "mcgarrah.org" || true)
          if [ -n "$INCONSISTENT" ]; then
            echo "‚ùå Inconsistent canonical URLs found:"
            echo "$INCONSISTENT"
            exit 1
          else
            echo "‚úÖ All canonical URLs consistent"
          fi
          
      - name: Validate sitemap.xml
        run: |
          echo "Validating sitemap.xml..."
          if [ -f "_site/sitemap.xml" ]; then
            # Check if sitemap is valid XML
            xmllint --noout _site/sitemap.xml
            echo "‚úÖ Sitemap XML is valid"
            
            # Check sitemap contains expected URLs
            URLS=$(grep -c "<url>" _site/sitemap.xml)
            echo "üìä Sitemap contains $URLS URLs"
            
            # Check for correct domain in sitemap
            WRONG_DOMAIN=$(grep -c "www.mcgarrah.org" _site/sitemap.xml || true)
            if [ "$WRONG_DOMAIN" -gt 0 ]; then
              echo "‚ùå Sitemap contains wrong domain (www.mcgarrah.org)"
              exit 1
            else
              echo "‚úÖ Sitemap uses correct domain"
            fi
          else
            echo "‚ùå Sitemap not found"
            exit 1
          fi
          
      - name: Check robots.txt
        run: |
          echo "Checking robots.txt..."
          if [ -f "_site/robots.txt" ]; then
            echo "‚úÖ Robots.txt exists"
            
            # Check sitemap reference in robots.txt
            if grep -q "Sitemap: https://mcgarrah.org/sitemap.xml" _site/robots.txt; then
              echo "‚úÖ Robots.txt references correct sitemap"
            else
              echo "‚ùå Robots.txt missing or incorrect sitemap reference"
              exit 1
            fi
          else
            echo "‚ùå Robots.txt not found"
            exit 1
          fi
          
      - name: Check meta tags on sample pages
        run: |
          echo "Checking meta tags..."
          
          # Check homepage
          if grep -q '<meta name="description"' _site/index.html; then
            echo "‚úÖ Homepage has meta description"
          else
            echo "‚ùå Homepage missing meta description"
            exit 1
          fi
          
          # Check for Open Graph tags
          if grep -q 'property="og:' _site/index.html; then
            echo "‚úÖ Homepage has Open Graph tags"
          else
            echo "‚ùå Homepage missing Open Graph tags"
            exit 1
          fi
          
      - name: Check for 404 page
        run: |
          echo "Checking 404 page..."
          if [ -f "_site/404.html" ]; then
            echo "‚úÖ Custom 404 page exists"
          else
            echo "‚ùå Custom 404 page missing"
            exit 1
          fi
          
      - name: Validate feed.xml
        run: |
          echo "Validating RSS feed..."
          if [ -f "_site/feed.xml" ]; then
            xmllint --noout _site/feed.xml
            echo "‚úÖ RSS feed XML is valid"
            
            # Check feed contains recent posts
            ITEMS=$(grep -c "<item>" _site/feed.xml || true)
            echo "üìä RSS feed contains $ITEMS items"
          else
            echo "‚ùå RSS feed not found"
            exit 1
          fi
          
      - name: Check links with Lychee
        uses: lycheeverse/lychee-action@v2.0.2
        with:
          args: --verbose --no-progress --exclude-path '_site/resume' '_site/**/*.html'
          fail: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Check for broken internal links (custom)
        run: |
          echo "Checking for broken internal links (Jekyll-specific)..."
          
          # Find all HTML files and check for internal links
          find _site -name "*.html" -exec grep -l 'href="/' {} \; | head -5 | while read file; do
            echo "Checking links in $file..."
            
            # Extract internal links and check if target files exist
            grep -o 'href="[^"]*"' "$file" | grep 'href="/' | sed 's/href="//;s/"//' | while read link; do
              # Remove anchor fragments
              clean_link=$(echo "$link" | sed 's/#.*//')
              
              # Skip external links, special cases, and /resume/ links (separate deployment)
              if [[ "$clean_link" =~ ^https?:// ]] || [[ "$clean_link" == "/" ]] || [[ "$clean_link" =~ ^/resume ]]; then
                continue
              fi
              
              # Check if file exists in _site
              target_file="_site${clean_link}"
              if [[ "$clean_link" == */ ]]; then
                target_file="${target_file}index.html"
              elif [[ ! "$clean_link" =~ \. ]]; then
                target_file="${target_file}/index.html"
              fi
              
              if [ ! -f "$target_file" ]; then
                echo "‚ö†Ô∏è  Potential broken link: $link in $file"
              fi
            done
          done
          
      - name: Check structured data
        run: |
          echo "Checking structured data..."
          
          # Check for JSON-LD structured data
          JSON_LD_PAGES=$(find _site -name "*.html" -exec grep -l 'application/ld+json' {} \; | wc -l)
          echo "üìä Pages with JSON-LD: $JSON_LD_PAGES"
          
          # Check homepage has structured data
          if grep -q 'application/ld+json' _site/index.html; then
            echo "‚úÖ Homepage has structured data"
          else
            echo "‚ö†Ô∏è  Homepage missing structured data"
          fi
          
      - name: Check image optimization
        run: |
          echo "Checking image optimization..."
          
          # Check for images without alt text
          MISSING_ALT=$(find _site -name "*.html" -exec grep -o '<img[^>]*>' {} \; | grep -v 'alt=' | wc -l || true)
          echo "‚ö†Ô∏è  Images without alt text: $MISSING_ALT"
          
          # Check for large images (basic check)
          find _site -name "*.jpg" -o -name "*.png" -o -name "*.jpeg" | while read img; do
            SIZE=$(stat -f%z "$img" 2>/dev/null || stat -c%s "$img" 2>/dev/null || echo 0)
            if [ "$SIZE" -gt 500000 ]; then  # 500KB
              echo "‚ö†Ô∏è  Large image: $img ($(($SIZE/1024))KB)"
            fi
          done
          
      - name: Check page performance indicators
        run: |
          echo "Checking performance indicators..."
          
          # Check for inline CSS (should be minimal)
          INLINE_CSS=$(find _site -name "*.html" -exec grep -o '<style[^>]*>.*</style>' {} \; | wc -l || true)
          echo "üìä Pages with inline CSS: $INLINE_CSS"
          
          # Check for external script count
          EXTERNAL_SCRIPTS=$(find _site -name "*.html" -exec grep -o 'src="http[^"]*"' {} \; | sort -u | wc -l || true)
          echo "üìä Unique external scripts: $EXTERNAL_SCRIPTS"
          
      - name: Check content quality indicators
        run: |
          echo "Checking content quality..."
          
          # Check for duplicate titles
          find _site -name "*.html" -exec grep -o '<title>[^<]*</title>' {} \; | sort | uniq -d > duplicate-titles.txt
          DUPLICATE_TITLES=$(wc -l < duplicate-titles.txt)
          if [ "$DUPLICATE_TITLES" -gt 0 ]; then
            echo "‚ö†Ô∏è  Duplicate titles found: $DUPLICATE_TITLES"
            head -5 duplicate-titles.txt
          else
            echo "‚úÖ No duplicate titles"
          fi
          
          # Check for duplicate meta descriptions
          find _site -name "*.html" -exec grep -l 'name="description"' {} \; | xargs grep -o 'name="description" content="[^"]*"' | sort | uniq -d > duplicate-descriptions.txt
          DUPLICATE_DESC=$(wc -l < duplicate-descriptions.txt)
          if [ "$DUPLICATE_DESC" -gt 0 ]; then
            echo "‚ö†Ô∏è  Duplicate meta descriptions: $DUPLICATE_DESC"
            echo "Files with duplicate descriptions:"
            find _site -name "*.html" -exec grep -l 'name="description"' {} \; | xargs grep -l "$(head -1 duplicate-descriptions.txt | sed 's/.*content="//;s/".*//')" | head -5
          else
            echo "‚úÖ No duplicate meta descriptions"
          fi
          
      - name: Check mobile optimization
        run: |
          echo "Checking mobile optimization..."
          
          # Check for viewport meta tag
          VIEWPORT_PAGES=$(find _site -name "*.html" -exec grep -l 'name="viewport"' {} \; | wc -l)
          TOTAL_PAGES=$(find _site -name "*.html" | wc -l)
          echo "üìä Pages with viewport meta: $VIEWPORT_PAGES/$TOTAL_PAGES"
          
          if [ "$VIEWPORT_PAGES" -eq "$TOTAL_PAGES" ]; then
            echo "‚úÖ All pages have viewport meta tag"
          else
            echo "‚ö†Ô∏è  Some pages missing viewport meta tag"
            echo "Pages missing viewport meta:"
            find _site -name "*.html" -exec grep -L 'name="viewport"' {} \; | head -5
          fi
          
      - name: Generate SEO report
        run: |
          echo "## SEO Health Check Report" > seo-report.md
          echo "Generated: $(date)" >> seo-report.md
          echo "" >> seo-report.md
          
          # Site statistics
          echo "### Site Statistics" >> seo-report.md
          echo "- Total HTML pages: $(find _site -name "*.html" | wc -l)" >> seo-report.md
          echo "- Sitemap URLs: $(grep -c "<url>" _site/sitemap.xml)" >> seo-report.md
          echo "- RSS feed items: $(grep -c "<item>" _site/feed.xml || echo "0")" >> seo-report.md
          echo "- Pages with JSON-LD: $(find _site -name "*.html" -exec grep -l 'application/ld+json' {} \; | wc -l)" >> seo-report.md
          echo "- Pages with viewport meta: $(find _site -name "*.html" -exec grep -l 'name="viewport"' {} \; | wc -l)" >> seo-report.md
          echo "" >> seo-report.md
          
          # Check for common SEO issues
          echo "### SEO Issues" >> seo-report.md
          
          # Pages without meta descriptions (excluding /assets/)
          MISSING_DESC=$(find _site -name "*.html" -not -path "_site/assets/*" -exec grep -L 'name="description"' {} \; | wc -l)
          echo "- Pages missing meta descriptions: $MISSING_DESC" >> seo-report.md
          if [ "$MISSING_DESC" -gt 0 ]; then
            echo "  Files missing descriptions:" >> seo-report.md
            find _site -name "*.html" -not -path "_site/assets/*" -exec grep -L 'name="description"' {} \; | head -5 | sed 's/^/    - /' >> seo-report.md
          fi
          
          # Pages without titles
          MISSING_TITLE=$(find _site -name "*.html" -exec grep -L '<title>' {} \; | wc -l)
          echo "- Pages missing titles: $MISSING_TITLE" >> seo-report.md
          if [ "$MISSING_TITLE" -gt 0 ]; then
            echo "  Files missing titles:" >> seo-report.md
            find _site -name "*.html" -exec grep -L '<title>' {} \; | head -5 | sed 's/^/    - /' >> seo-report.md
          fi
          
          # Images without alt text
          MISSING_ALT=$(find _site -name "*.html" -exec grep -o '<img[^>]*>' {} \; | grep -v 'alt=' | wc -l || true)
          echo "- Images without alt text: $MISSING_ALT" >> seo-report.md
          
          # Duplicate content issues
          DUPLICATE_TITLES=$(find _site -name "*.html" -exec grep -o '<title>[^<]*</title>' {} \; | sort | uniq -d | wc -l)
          echo "- Duplicate titles: $DUPLICATE_TITLES" >> seo-report.md
          
          echo "" >> seo-report.md
          echo "‚úÖ SEO health check completed successfully" >> seo-report.md
          
          cat seo-report.md
          
      - name: Display SEO report
        run: |
          echo "üìã SEO Health Check Report Contents:"
          echo "==========================================="
          cat seo-report.md
          echo "==========================================="
          
      - name: Upload SEO report
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          echo "ARTIFACT_NAME=seo-health-report-$TIMESTAMP" >> $GITHUB_ENV
        
      - name: Upload SEO report artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME }}
          path: seo-report.md
          retention-days: 90
